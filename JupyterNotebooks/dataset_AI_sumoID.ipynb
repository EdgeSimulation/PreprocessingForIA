{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5aa9913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Copyright (C) 2024 by Sonja Filiposka <sonja.filiposka@finki.ukim.mk>\n",
    "#\n",
    "# This code is licensed under a Creative Commons Attribution 4.0 International License. (see LICENSE.txt for details)\n",
    "#\n",
    "# [TODO]General Description - this notebook is used to extract the delay and the handover information from OMNET output vector file\n",
    "# It creates two types of output files: \n",
    "#    - mobile network communication delay as reported by OMNET\n",
    "#      - a csv file with the delay for each communication exchange\n",
    "#    - initial positioning and migration files that are used as input for CloudSim \n",
    "#       - initial positioning file defines the start and end time for each car/service and the initial community \n",
    "#         based on location of tha car and the location of the base stations (see comments in code)\n",
    "#       - migration file defines the time stamp when a car moves from one community to another based on relative location to the nearest base station\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f225bd91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dataset_sumo(vector_sumo, output):\n",
    "\n",
    "    # parameters\n",
    "    # 1 = $vector_sumo - sumo input.file name\n",
    "    # 2 = $output      - output.file name\n",
    "\n",
    "    # sumo log format:\n",
    "    #   <timestep time=\"0.00\">\n",
    "    #     <vehicle id=\"0\" x=\"-0.482437\" y=\"38.344131\" angle=\"339.66\" type=\"DEFAULT_VEHTYPE\" speed=\"5.10\" pos=\"5.10\" lane=\"23036317#1_0\" slope=\"0.00\" signals=\"0\"/>\n",
    "    !grep -n 'time=' $vector_sumo | tr -d \\\" > sumo_times.txt\n",
    "    !grep -n 'vehicle' $vector_sumo | tr -d \\\" > sumo_veh.txt\n",
    "\n",
    "    cols=['row','veh_id','x','y','angle','speed','pos','lane','slope','signals']\n",
    "    data = vx.read_csv(\"sumo_veh.txt\", sep=' ', header=None,\n",
    "            names=['row','1','2','3','4','5','6','7','8','veh_id','x','y','angle','type','speed','pos','lane','slope','signals'], \n",
    "            usecols=cols,\n",
    "            convert=True, chunk_size=150_000_000)    \n",
    "    #print(data.shape, data)\n",
    "\n",
    "    # clean vehicle's file\n",
    "    !rm sumo_veh.txt \n",
    "    \n",
    "    data['veh_id'] = data['veh_id'].str.replace('id=', '').str.replace('\"', '')\n",
    "    data['x'] = data[\"x\"].str.replace('x=', '')\n",
    "    data['y'] = data[\"y\"].str.replace('y=', '')\n",
    "    data['angle'] = data[\"angle\"].str.replace('angle=', '')\n",
    "    data['speed'] = data[\"speed\"].str.replace('speed=', '')\n",
    "    data['pos'] = data[\"pos\"].str.replace('pos=', '')\n",
    "    data['lane'] = data[\"lane\"].str.replace('lane=', '')\n",
    "    data['slope'] = data[\"slope\"].str.replace('slope=', '')\n",
    "    data['signals'] = data[\"signals\"].str.replace('signals=', '').str.replace('/>', '') \n",
    "    data['row'] = data[\"row\"].str.replace(':', '').astype('int')\n",
    "           \n",
    "    data_t = pd.read_csv(\"sumo_times.txt\", sep=' ', header=None,\n",
    "            names=['row','1','2','3','4','time'], \n",
    "            usecols=['row','time'])\n",
    "    data_t['time'] = data_t['time'].str.replace('time=', '').str.replace('>', '').astype('float')\n",
    "    data_t['row'] = data_t['row'].str.replace(':', '').astype('int')  \n",
    "    # check\n",
    "    #print(data_t,data_t.shape,data_t.row.dtype, data_t.time.dtype)\n",
    "    data_du = data[\"veh_id\"].nunique()\n",
    "    print('\\n(1) unique:',data_du)\n",
    "\n",
    "    # clean times file\n",
    "    !rm sumo_times.txt\n",
    "\n",
    "    # difference of <timestep time=... rows\n",
    "    # 3:  1 row of <vehicle id= ...\n",
    "    # 4:  2 rows of <vehicle id= ...\n",
    "    # ....\n",
    "    # 90: 88 rows of <vehicle id= ...\n",
    "    data_t['row_diff'] = data_t.row.diff()\n",
    "    print(data_t, data.shape)    \n",
    "    \n",
    "    # build the dictionary\n",
    "    d = {}\n",
    "     \n",
    "    # get times\n",
    "    for idx, row in data_t.iterrows():\n",
    "        i = row.row_diff - 2\n",
    "        row_ant = data_t.iloc[idx-1].row; time_ant = data_t.iloc[idx-1].time\n",
    "        row_this = data_t.iloc[idx].row;  time = data_t.iloc[idx].time\n",
    "        while (i > 0):\n",
    "            d[int(row_ant + i)] = time_ant\n",
    "            i = i - 1\n",
    "        # if last row\n",
    "        if idx == (data_t.shape[0]-1):\n",
    "            i = row.row_diff - 2\n",
    "            while (i > 0):\n",
    "                d[int(row_this + i)] = time\n",
    "                i = i - 1\n",
    "        # a way to save memory, otherwise it overflows\n",
    "        gap = 50000\n",
    "        if (idx % gap == gap-1):\n",
    "            label = str(int(idx/gap))\n",
    "            data['t' + label] = data.row.map(d, default_value = 0.0)    \n",
    "            d = {}\n",
    "        \n",
    "    # get the rest of 'time' values\n",
    "    label = str(int(idx/gap))\n",
    "    data['t' + label]  = data.row.map(d, default_value = 0.0)      \n",
    "\n",
    "    # merge time columns (max 4 columns for 1800s)\n",
    "    data['t'] = data.t0 + data.t1 + data.t2 + data.t3        \n",
    "    \n",
    "    data = data[['t','veh_id','x','y','angle','speed','pos','lane','slope','signals']]\n",
    "    data_du = data[\"veh_id\"].nunique()\n",
    "    print(data, '\\n(2) unique:',data_du)\n",
    "\n",
    "    data.export_csv(output, index=False, sep='\\t')\n",
    "    \n",
    "    del data_t, data\n",
    "    \n",
    "    return 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02007059-72ee-484e-8873-bd380b38c19d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dataset_omnet(vector_omnet, output):\n",
    "\n",
    "    # parameters\n",
    "    # 1 = $vector_omnet - omnet input.file name\n",
    "    # 2 = $output       - output.file name\n",
    "        \n",
    "    # First prepare the output file from OMNET, vector-0.vec to suit version 2.0\n",
    "    !split -d -b 500m $vector_omnet\n",
    "    !sed -i 's/version 3/version 2/' x00\n",
    "    !cat x* > vector-1.vec\n",
    "    # clean\n",
    "    !rm x*\n",
    "    # requires to have netperfmeter installed: sudo apt-get install netperfmeter\n",
    "    !extractvectors vector-1.vec results.bz2 \"distance\" \\\n",
    "        \"measuredSinrDl\" \"measuredSinrUl\" \"rcvdSinrDl\" \"averageCqiDl\" \\\n",
    "        \"servingCell\" \"rlcDelayDl\" \"rlcPacketLossTotal\" \"rlcPduDelayDl\" \\\n",
    "        \"rlcPduPacketLossDl\" \"rlcPduThroughputDl\" \"rlcThroughputDl\"\\\n",
    "        \"receivedPacketFromLowerLayer\" \n",
    "\n",
    "    # get data file: results (it deletes results.bz2)\n",
    "    if os.path.isfile('results'):\n",
    "        !rm results\n",
    "    !bzip2 -d results.bz2\n",
    "    \n",
    "    # removing unnecessary information\n",
    "    !grep 'car' results > r_cars\n",
    "    !sed -i -e 's/NRSeveralBSALC.//' -e 's/:vector//'  -e 's/ ETV//' \\\n",
    "        -e 's/(packetBytes)//' r_cars\n",
    "    \n",
    "    # clean\n",
    "    !rm vector-1.vec \n",
    "    \n",
    "    cols=['Time','Event','Object','Vector','Split','Value']\n",
    "    data = vx.read_csv(\"r_cars\", sep='\\t', header=None,\n",
    "        #names=['#','Time','Event','Object','Vector','Split','Value'],\n",
    "        names=['#','Time','Event','Object','Vector','Value'],\n",
    "        usecols=['Time','Object','Vector','Value'],\n",
    "        convert=True, chunk_size=150_000_000) \n",
    "    print(data)\n",
    "\n",
    "    #data = data['#'].drop\n",
    "    data.Object = data.Object.str.replace('\"', '')\n",
    "    data.Vector = data.Vector.str.replace('\"', '')    \n",
    "\n",
    "    data.export_csv(output, index=False, sep='\\t')\n",
    "    \n",
    "    # clean\n",
    "    !rm results r_cars* \n",
    "    del data\n",
    "    \n",
    "    return 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af2032d6-5724-4948-82a1-ac768fa044fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#with pandas: for files smaller than 1 GB (and faster than vaex)\n",
    "def process_dataset_sumo_future(s_input, output):\n",
    "    df_pd = pd.read_csv(s_input, sep='\\t', header=None,\n",
    "            names=['t','veh_id','x','y','angle','speed','pos','lane','slope','signals']) \n",
    "            #header=1)\n",
    "    print(df_pd.head())\n",
    "    print(df_pd['veh_id'])\n",
    "    df_pd=df_pd.tail(-1)\n",
    "\n",
    "    \n",
    "    # with pandas: it is ok if input file is smaller than 1 GB\n",
    "    df_pd['x1'] = df_pd.groupby('veh_id')['x'].shift(-100).fillna(999999)\n",
    "    df_pd['y1'] = df_pd.groupby('veh_id')['y'].shift(-100).fillna(999999)\n",
    "    df_pd['x2'] = df_pd.groupby('veh_id')['x'].shift(-200).fillna(999999)\n",
    "    df_pd['y2'] = df_pd.groupby('veh_id')['y'].shift(-200).fillna(999999)\n",
    "    df_pd['x3'] = df_pd.groupby('veh_id')['x'].shift(-300).fillna(999999)\n",
    "    df_pd['y3'] = df_pd.groupby('veh_id')['y'].shift(-300).fillna(999999)\n",
    "    df_pd['x4'] = df_pd.groupby('veh_id')['x'].shift(-400).fillna(999999)\n",
    "    df_pd['y4'] = df_pd.groupby('veh_id')['y'].shift(-400).fillna(999999)\n",
    "    df_pd['x5'] = df_pd.groupby('veh_id')['x'].shift(-500).fillna(999999)\n",
    "    df_pd['y5'] = df_pd.groupby('veh_id')['y'].shift(-500).fillna(999999)\n",
    "    df_pd['x6'] = df_pd.groupby('veh_id')['x'].shift(-600).fillna(999999)\n",
    "    df_pd['y6'] = df_pd.groupby('veh_id')['y'].shift(-600).fillna(999999)\n",
    "    df_pd['x7'] = df_pd.groupby('veh_id')['x'].shift(-700).fillna(999999)\n",
    "    df_pd['y7'] = df_pd.groupby('veh_id')['y'].shift(-700).fillna(999999)\n",
    "    print(df_pd.shape, df_pd)\n",
    "\n",
    "    df_pd.to_csv(output, index=False, sep='\\t')\n",
    "\n",
    "    return 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "532d90ea-f8c4-40e7-845f-a3b0784f411c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#with pandas: for files smaller than 1 GB (and faster than vaex)\n",
    "def process_dataset_sumo_past(s_input, output):\n",
    "    df_pd = pd.read_csv(s_input, sep='\\t', header=None,\n",
    "            names=['t','veh_id','x','y','angle','speed','pos','lane','slope','signals']) \n",
    "            #header=1)\n",
    "    print(df_pd.shape)\n",
    "    print(df_pd['veh_id'])\n",
    "    df_pd=df_pd.tail(-1)\n",
    "    print(df_pd.head())\n",
    "    \n",
    "    # with pandas: it is ok if input file is smaller than 1 GB\n",
    "    df_pd['x-1'] = df_pd.groupby('veh_id')['x'].shift(100).fillna(999999)\n",
    "    df_pd['y-1'] = df_pd.groupby('veh_id')['y'].shift(100).fillna(999999)\n",
    "    df_pd['x-2'] = df_pd.groupby('veh_id')['x'].shift(200).fillna(999999)\n",
    "    df_pd['y-2'] = df_pd.groupby('veh_id')['y'].shift(200).fillna(999999)\n",
    "    df_pd['x-3'] = df_pd.groupby('veh_id')['x'].shift(300).fillna(999999)\n",
    "    df_pd['y-3'] = df_pd.groupby('veh_id')['y'].shift(300).fillna(999999)\n",
    "    df_pd['x-4'] = df_pd.groupby('veh_id')['x'].shift(400).fillna(999999)\n",
    "    df_pd['y-4'] = df_pd.groupby('veh_id')['y'].shift(400).fillna(999999)\n",
    "    df_pd['x-5'] = df_pd.groupby('veh_id')['x'].shift(500).fillna(999999)\n",
    "    df_pd['y-5'] = df_pd.groupby('veh_id')['y'].shift(500).fillna(999999)\n",
    "    df_pd['x-6'] = df_pd.groupby('veh_id')['x'].shift(600).fillna(999999)\n",
    "    df_pd['y-6'] = df_pd.groupby('veh_id')['y'].shift(600).fillna(999999)\n",
    "    df_pd['x-7'] = df_pd.groupby('veh_id')['x'].shift(700).fillna(999999)\n",
    "    df_pd['y-7'] = df_pd.groupby('veh_id')['y'].shift(700).fillna(999999)\n",
    "    print(df_pd.shape, df_pd)\n",
    "\n",
    "    df_pd.to_csv(output, index=False, sep='\\t')\n",
    "\n",
    "    return 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537a1dca",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2) sumo input :  dataset_AI_output/8589_sumo_AI.csv\n",
      "STEP 2) sumo output :  dataset_AI_output/8589_sumo_2_AI.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5809/1462651122.py:3: DtypeWarning: Columns (0,1,2,3,4,5,6,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_pd = pd.read_csv(s_input, sep='\\t', header=None,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20679146, 10)\n",
      "0           veh_id\n",
      "1                0\n",
      "2                0\n",
      "3                0\n",
      "4                0\n",
      "             ...  \n",
      "20679141       888\n",
      "20679142       962\n",
      "20679143       972\n",
      "20679144       975\n",
      "20679145       999\n",
      "Name: veh_id, Length: 20679146, dtype: object\n",
      "      t veh_id          x          y  angle speed   pos        lane slope  \\\n",
      "1   0.0      0  -0.485544  38.341737  39.92  0.00  5.10  33199229_0  0.00   \n",
      "2  0.01      0  -0.485544  38.341737  39.92  0.03  5.10  33199229_0  0.00   \n",
      "3  0.02      0  -0.485544  38.341737  39.92  0.05  5.10  33199229_0  0.00   \n",
      "4  0.03      0  -0.485544  38.341737  39.92  0.08  5.10  33199229_0  0.00   \n",
      "5  0.04      0  -0.485544  38.341737  39.92  0.10  5.10  33199229_0  0.00   \n",
      "\n",
      "  signals  \n",
      "1       0  \n",
      "2       8  \n",
      "3       8  \n",
      "4       8  \n",
      "5       0  \n",
      "(20679145, 24) "
     ]
    }
   ],
   "source": [
    "# measuring execution time\n",
    "%load_ext autotime\n",
    "\n",
    "# extract the delay and the handover information from omnet output vector file\n",
    "maxTime = 1800\n",
    "communities = 9\n",
    "\n",
    "#parametrised calls of notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import vaex as vx\n",
    "import os.path\n",
    "\n",
    "path=\"/home/jupyter/notebook/OMNET6.0/\"\n",
    "sumo_files = \"dataset_AI_input/fdc_signals_\"\n",
    "omnet_files = \"VoipDl-Urban-xxxx/VoipDl-Urban-\"\n",
    "#cars = np.array([4928, 4951, 4955, 5712, 5734, 5749, 6900, 6908, 6923, 8589, 8619, 8620])\n",
    "cars = np.array([8589, 8619, 8620])\n",
    "out = \"dataset_AI_output/\"\n",
    "outFile = \"_AI.csv\"\n",
    "# initialPositioning-xxxx.txt and migrations-xxxx.txt will also be.ipynb_checkpoints/created\n",
    "\n",
    "for i in cars:\n",
    "    # sumo datasets are already created (based on fdc files)\n",
    "    # info about cars positioning and parameters over time\n",
    "    # obtained from sumo with commands:\n",
    "    # sumo -c Alicante_8620.sumo.cfg --fcd-output.geo true --fcd-output.signals true --fcd-output ../fdc_signals_8620.xml --end 1800\n",
    "    v_sumo = path + sumo_files + str(i) +  \".xml\"\n",
    "    output = out + str(i) + \"_sumo\" + outFile\n",
    "    #print(\"STEP 1) sumo input : \", v_sumo)\n",
    "    #print(\"STEP 1) sumo output : \", output)\n",
    "    # get sumo dataset\n",
    "    #df_sumo_exit_code = dataset_sumo(v_sumo, output)\n",
    "\n",
    "    # process sumo dataset to get future positions\n",
    "    #v_sumo = output\n",
    "    #output = out + str(i) + \"_sumo_1\" + outFile\n",
    "    #print(\"STEP 2) sumo input : \", v_sumo)\n",
    "    #print(\"STEP 2) sumo output : \", output)\n",
    "    #df_sumo_exit_code = process_dataset_sumo_future(v_sumo, output)\n",
    "    # check\n",
    "    #print(df_sumo_exit_code)\n",
    "    \n",
    "    # process sumo dataset to get past positions\n",
    "    v_sumo = output\n",
    "    output = out + str(i) + \"_sumo_2\" + outFile\n",
    "    print(\"STEP 2) sumo input : \", v_sumo)\n",
    "    print(\"STEP 2) sumo output : \", output)\n",
    "    df_sumo_exit_code = process_dataset_sumo_past(v_sumo, output)\n",
    "    # check\n",
    "    #print(df_sumo_exit_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a13405-59bd-43f1-bc3e-a9fca6202122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674acfba-0313-4a2a-b07d-7ef9f42ea9d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
