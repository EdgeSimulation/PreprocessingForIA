{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aa9913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Copyright (C) 2024 by Katja Gilly <katya@umh.es>\n",
    "#\n",
    "# This code is licensed under a Creative Commons Attribution 4.0 International License. (see LICENSE.txt for details)\n",
    "#\n",
    "# General Description - this notebook is used to extract data from SUMO logs in a table format indexed by time and vehicle id.\n",
    "# It creates three types of output files: \n",
    "#    - (STEP 1) a csv file per SUMO log formatted as a table with most atributes and signals per vehicle.\n",
    "#    - (STEP 2) a csv file that adds the 7 previous geo positions of vehicles in each row. It requires (STEP 1) to be run previously.\n",
    "#    - (STEP 3) a csv file that adds the 7 past geo positions of vehicles in each row. It requires (STEP 1) to be run previously.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f225bd91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dataset_sumo(vector_sumo, output):\n",
    "\n",
    "    # parameters\n",
    "    # 1 = $vector_sumo - sumo input.file name\n",
    "    # 2 = $output      - output.file name\n",
    "\n",
    "    # sumo log format:\n",
    "    #   <timestep time=\"0.00\">\n",
    "    #     <vehicle id=\"0\" x=\"-0.482437\" y=\"38.344131\" angle=\"339.66\" type=\"DEFAULT_VEHTYPE\" speed=\"5.10\" pos=\"5.10\" lane=\"23036317#1_0\" slope=\"0.00\" signals=\"0\"/>\n",
    "    !grep -n 'time=' $vector_sumo | tr -d \\\" > sumo_times.txt\n",
    "    !grep -n 'vehicle' $vector_sumo | tr -d \\\" > sumo_veh.txt\n",
    "\n",
    "    cols=['row','veh_id','x','y','angle','speed','pos','lane','slope','signals']\n",
    "    data = vx.read_csv(\"sumo_veh.txt\", sep=' ', header=None,\n",
    "            names=['row','1','2','3','4','5','6','7','8','veh_id','x','y','angle','type','speed','pos','lane','slope','signals'], \n",
    "            usecols=cols,\n",
    "            convert=True, chunk_size=150_000_000)    \n",
    "    #print(data.shape, data)\n",
    "\n",
    "    # clean vehicle's file\n",
    "    !rm sumo_veh.txt \n",
    "    \n",
    "    data['veh_id'] = data['veh_id'].str.replace('id=', '').str.replace('\"', '')\n",
    "    data['x'] = data[\"x\"].str.replace('x=', '')\n",
    "    data['y'] = data[\"y\"].str.replace('y=', '')\n",
    "    data['angle'] = data[\"angle\"].str.replace('angle=', '')\n",
    "    data['speed'] = data[\"speed\"].str.replace('speed=', '')\n",
    "    data['pos'] = data[\"pos\"].str.replace('pos=', '')\n",
    "    data['lane'] = data[\"lane\"].str.replace('lane=', '')\n",
    "    data['slope'] = data[\"slope\"].str.replace('slope=', '')\n",
    "    data['signals'] = data[\"signals\"].str.replace('signals=', '').str.replace('/>', '') \n",
    "    data['row'] = data[\"row\"].str.replace(':', '').astype('int')\n",
    "           \n",
    "    data_t = pd.read_csv(\"sumo_times.txt\", sep=' ', header=None,\n",
    "            names=['row','1','2','3','4','time'], \n",
    "            usecols=['row','time'])\n",
    "    data_t['time'] = data_t['time'].str.replace('time=', '').str.replace('>', '').astype('float')\n",
    "    data_t['row'] = data_t['row'].str.replace(':', '').astype('int')  \n",
    "    # check\n",
    "    #print(data_t,data_t.shape,data_t.row.dtype, data_t.time.dtype)\n",
    "    data_du = data[\"veh_id\"].nunique()\n",
    "    print('\\n(1) unique:',data_du)\n",
    "\n",
    "    # clean times file\n",
    "    !rm sumo_times.txt\n",
    "\n",
    "    # difference of <timestep time=... rows\n",
    "    # 3:  1 row of <vehicle id= ...\n",
    "    # 4:  2 rows of <vehicle id= ...\n",
    "    # ....\n",
    "    # 90: 88 rows of <vehicle id= ...\n",
    "    data_t['row_diff'] = data_t.row.diff()\n",
    "    print(data_t, data.shape)    \n",
    "    \n",
    "    # build the dictionary\n",
    "    d = {}\n",
    "     \n",
    "    # get times\n",
    "    for idx, row in data_t.iterrows():\n",
    "        i = row.row_diff - 2\n",
    "        row_ant = data_t.iloc[idx-1].row; time_ant = data_t.iloc[idx-1].time\n",
    "        row_this = data_t.iloc[idx].row;  time = data_t.iloc[idx].time\n",
    "        while (i > 0):\n",
    "            d[int(row_ant + i)] = time_ant\n",
    "            i = i - 1\n",
    "        # if last row\n",
    "        if idx == (data_t.shape[0]-1):\n",
    "            i = row.row_diff - 2\n",
    "            while (i > 0):\n",
    "                d[int(row_this + i)] = time\n",
    "                i = i - 1\n",
    "        # a way to save memory, otherwise it overflows\n",
    "        gap = 50000\n",
    "        if (idx % gap == gap-1):\n",
    "            label = str(int(idx/gap))\n",
    "            data['t' + label] = data.row.map(d, default_value = 0.0)    \n",
    "            d = {}\n",
    "        \n",
    "    # get the rest of 'time' values\n",
    "    label = str(int(idx/gap))\n",
    "    data['t' + label]  = data.row.map(d, default_value = 0.0)      \n",
    "\n",
    "    # merge time columns (max 4 columns for 1800s)\n",
    "    data['t'] = data.t0 + data.t1 + data.t2 + data.t3        \n",
    "    \n",
    "    data = data[['t','veh_id','x','y','angle','speed','pos','lane','slope','signals']]\n",
    "    data_du = data[\"veh_id\"].nunique()\n",
    "    print(data, '\\n(2) unique:',data_du)\n",
    "\n",
    "    data.export_csv(output, index=False, sep='\\t')\n",
    "    \n",
    "    del data_t, data\n",
    "    \n",
    "    return 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af2032d6-5724-4948-82a1-ac768fa044fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#with pandas: for files smaller than 1 GB (and faster than vaex)\n",
    "def process_dataset_sumo_future(s_input, output):\n",
    "    df_pd = pd.read_csv(s_input, sep='\\t', header=None,\n",
    "            names=['t','veh_id','x','y','angle','speed','pos','lane','slope','signals']) \n",
    "            #header=1)\n",
    "    print(df_pd.head())\n",
    "    print(df_pd['veh_id'])\n",
    "    df_pd=df_pd.tail(-1)\n",
    "\n",
    "    \n",
    "    # with pandas: it is ok if input file is smaller than 1 GB\n",
    "    df_pd['x1'] = df_pd.groupby('veh_id')['x'].shift(-100).fillna(999999)\n",
    "    df_pd['y1'] = df_pd.groupby('veh_id')['y'].shift(-100).fillna(999999)\n",
    "    df_pd['x2'] = df_pd.groupby('veh_id')['x'].shift(-200).fillna(999999)\n",
    "    df_pd['y2'] = df_pd.groupby('veh_id')['y'].shift(-200).fillna(999999)\n",
    "    df_pd['x3'] = df_pd.groupby('veh_id')['x'].shift(-300).fillna(999999)\n",
    "    df_pd['y3'] = df_pd.groupby('veh_id')['y'].shift(-300).fillna(999999)\n",
    "    df_pd['x4'] = df_pd.groupby('veh_id')['x'].shift(-400).fillna(999999)\n",
    "    df_pd['y4'] = df_pd.groupby('veh_id')['y'].shift(-400).fillna(999999)\n",
    "    df_pd['x5'] = df_pd.groupby('veh_id')['x'].shift(-500).fillna(999999)\n",
    "    df_pd['y5'] = df_pd.groupby('veh_id')['y'].shift(-500).fillna(999999)\n",
    "    df_pd['x6'] = df_pd.groupby('veh_id')['x'].shift(-600).fillna(999999)\n",
    "    df_pd['y6'] = df_pd.groupby('veh_id')['y'].shift(-600).fillna(999999)\n",
    "    df_pd['x7'] = df_pd.groupby('veh_id')['x'].shift(-700).fillna(999999)\n",
    "    df_pd['y7'] = df_pd.groupby('veh_id')['y'].shift(-700).fillna(999999)\n",
    "    print(df_pd.shape, df_pd)\n",
    "\n",
    "    df_pd.to_csv(output, index=False, sep='\\t')\n",
    "\n",
    "    return 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "532d90ea-f8c4-40e7-845f-a3b0784f411c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#with pandas: for files smaller than 1 GB (and faster than vaex)\n",
    "def process_dataset_sumo_past(s_input, output):\n",
    "    df_pd = pd.read_csv(s_input, sep='\\t', header=None,\n",
    "            names=['t','veh_id','x','y','angle','speed','pos','lane','slope','signals']) \n",
    "            #header=1)\n",
    "    print(df_pd.shape)\n",
    "    print(df_pd['veh_id'])\n",
    "    df_pd=df_pd.tail(-1)\n",
    "    print(df_pd.head())\n",
    "    \n",
    "    # with pandas: it is ok if input file is smaller than 1 GB\n",
    "    df_pd['x-1'] = df_pd.groupby('veh_id')['x'].shift(100).fillna(999999)\n",
    "    df_pd['y-1'] = df_pd.groupby('veh_id')['y'].shift(100).fillna(999999)\n",
    "    df_pd['x-2'] = df_pd.groupby('veh_id')['x'].shift(200).fillna(999999)\n",
    "    df_pd['y-2'] = df_pd.groupby('veh_id')['y'].shift(200).fillna(999999)\n",
    "    df_pd['x-3'] = df_pd.groupby('veh_id')['x'].shift(300).fillna(999999)\n",
    "    df_pd['y-3'] = df_pd.groupby('veh_id')['y'].shift(300).fillna(999999)\n",
    "    df_pd['x-4'] = df_pd.groupby('veh_id')['x'].shift(400).fillna(999999)\n",
    "    df_pd['y-4'] = df_pd.groupby('veh_id')['y'].shift(400).fillna(999999)\n",
    "    df_pd['x-5'] = df_pd.groupby('veh_id')['x'].shift(500).fillna(999999)\n",
    "    df_pd['y-5'] = df_pd.groupby('veh_id')['y'].shift(500).fillna(999999)\n",
    "    df_pd['x-6'] = df_pd.groupby('veh_id')['x'].shift(600).fillna(999999)\n",
    "    df_pd['y-6'] = df_pd.groupby('veh_id')['y'].shift(600).fillna(999999)\n",
    "    df_pd['x-7'] = df_pd.groupby('veh_id')['x'].shift(700).fillna(999999)\n",
    "    df_pd['y-7'] = df_pd.groupby('veh_id')['y'].shift(700).fillna(999999)\n",
    "    print(df_pd.shape, df_pd)\n",
    "\n",
    "    df_pd.to_csv(output, index=False, sep='\\t')\n",
    "\n",
    "    return 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "537a1dca",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1) sumo input :  /home/jupyter/notebook/OMNET6.0//dataset_AI_input/fdc_signals_4928.xml\n",
      "STEP 1) sumo output :  dataset_AI_output/4928_sumo_AI.csv\n",
      "\n",
      "(1) unique: 887\n",
      "             row     time  row_diff\n",
      "0             38     0.00       NaN\n",
      "1             41     0.01       3.0\n",
      "2             44     0.02       3.0\n",
      "3             47     0.03       3.0\n",
      "4             50     0.04       3.0\n",
      "...          ...      ...       ...\n",
      "179995  10231732  1799.95      90.0\n",
      "179996  10231822  1799.96      90.0\n",
      "179997  10231912  1799.97      90.0\n",
      "179998  10232002  1799.98      90.0\n",
      "179999  10232092  1799.99      90.0\n",
      "\n",
      "[180000 rows x 3 columns] (9872144, 10)\n",
      "#          t        veh_id    x          y          angle    speed    pos    lane             slope    signals\n",
      "0          0.0      0         -0.482437  38.344131  339.66   0.00     5.10   23036317#1_0     0.00     0\n",
      "1          0.01     0         -0.482437  38.344131  339.66   0.03     5.10   23036317#1_0     0.00     8\n",
      "2          0.02     0         -0.482437  38.344131  339.66   0.05     5.10   23036317#1_0     0.00     8\n",
      "3          0.03     0         -0.482437  38.344131  339.66   0.08     5.10   23036317#1_0     0.00     8\n",
      "4          0.04     0         -0.482437  38.344131  339.66   0.10     5.10   23036317#1_0     0.00     0\n",
      "...        ...      ...       ...        ...        ...      ...      ...    ...              ...      ...\n",
      "9,872,139  1799.99  991       -0.489463  38.340860  149.02   10.90    25.62  447725242#14_0   0.00     0\n",
      "9,872,140  1799.99  992       -0.494453  38.344146  346.10   0.00     35.98  20490204#2_0     0.00     8\n",
      "9,872,141  1799.99  994       -0.489472  38.346194  149.79   10.08    7.89   22277431#1_0     0.00     0\n",
      "9,872,142  1799.99  995       -0.486905  38.345166  162.91   13.80    0.63   22278655#9_0     0.00     0\n",
      "9,872,143  1799.99  996       -0.488680  38.345076  148.64   6.34     0.34   :4547207859_0_0  0.00     8 \n",
      "(2) unique: 887\n"
     ]
    }
   ],
   "source": [
    "# measuring execution time\n",
    "%load_ext autotime\n",
    "\n",
    "# extract the delay and the handover information from omnet output vector file\n",
    "maxTime = 1800\n",
    "communities = 9\n",
    "\n",
    "#parametrised calls of notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import vaex as vx\n",
    "import os.path\n",
    "\n",
    "path=\"/home/jupyter/notebook/OMNET6.0/\"\n",
    "sumo_files = \"/dataset_AI_input/fdc_signals_\"\n",
    "#cars = np.array([4928, 4951, 4955, 5712, 5734, 5749, 6900, 6908, 6923, 8589, 8619, 8620])\n",
    "cars = np.array([4928])\n",
    "out = \"dataset_AI_output/\"\n",
    "outFile = \"_AI.csv\"\n",
    "# initialPositioning-xxxx.txt and migrations-xxxx.txt will also be.ipynb_checkpoints/created\n",
    "\n",
    "for i in cars:\n",
    "    # sumo datasets are already created (based on fdc files)\n",
    "    # info about cars positioning and parameters over time\n",
    "    # obtained from sumo with commands:\n",
    "    # sumo -c Alicante_8620.sumo.cfg --fcd-output.geo true --fcd-output.signals true --fcd-output ../fdc_signals_8620.xml --end 1800\n",
    "    v_sumo = path + sumo_files + str(i) +  \".xml\"\n",
    "    output = out + str(i) + \"_sumo\" + outFile\n",
    "    print(\"STEP 1) sumo input : \", v_sumo)\n",
    "    print(\"STEP 1) sumo output : \", output)\n",
    "    # get sumo dataset\n",
    "    df_sumo_exit_code = dataset_sumo(v_sumo, output)\n",
    "\n",
    "    # process sumo dataset to get future positions\n",
    "    #v_sumo = output\n",
    "    #output = out + str(i) + \"_sumo_1\" + outFile\n",
    "    #print(\"STEP 2) sumo input : \", v_sumo)\n",
    "    #print(\"STEP 2) sumo output : \", output)\n",
    "    #df_sumo_exit_code = process_dataset_sumo_future(v_sumo, output)\n",
    "    # check\n",
    "    #print(df_sumo_exit_code)\n",
    "    \n",
    "    # process sumo dataset to get past positions\n",
    "    #v_sumo = output\n",
    "    #output = out + str(i) + \"_sumo_2\" + outFile\n",
    "    #print(\"STEP 3) sumo input : \", v_sumo)\n",
    "    #print(\"STEP 3) sumo output : \", output)\n",
    "    #df_sumo_exit_code = process_dataset_sumo_past(v_sumo, output)\n",
    "    # check\n",
    "    #print(df_sumo_exit_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a13405-59bd-43f1-bc3e-a9fca6202122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674acfba-0313-4a2a-b07d-7ef9f42ea9d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
